#include "plssvm/backends/gpu_csvm.hpp"

#include "plssvm/constants.hpp"               // plssvm::THREAD_BLOCK_SIZE, plssvm::INTERNAL_BLOCK_SIZE
#include "plssvm/csvm.hpp"                    // plssvm::csvm
#include "plssvm/detail/execution_range.hpp"  // plssvm::detail::execution_range
#include "plssvm/detail/operators.hpp"        // various operator overloads for std::vector and scalars
#include "plssvm/exceptions/exceptions.hpp"   // plssvm::exception
#include "plssvm/parameter.hpp"               // plssvm::parameter

#if defined(PLSSVM_HAS_CUDA_BACKEND)
    // used for explicitly instantiating the CUDA backend
    #include "plssvm/backends/CUDA/detail/device_ptr.cuh"
#endif
#if defined(PLSSVM_HAS_HIP_BACKEND)
    // used for explicitly instantiating the HIP backend
    #include "plssvm/backends/HIP/detail/device_ptr.hip.hpp"
#endif
#if defined(PLSSVM_HAS_OPENCL_BACKEND)
    // used for explicitly instantiating the OpenCL backend
    #include "plssvm/backends/OpenCL/detail/command_queue.hpp"
    #include "plssvm/backends/OpenCL/detail/device_ptr.hpp"
#endif
#if defined(PLSSVM_HAS_SYCL_BACKEND)
    // used for explicitly instantiating the SYCL backend
    #include "sycl/sycl.hpp"
#if defined(PLSSVM_SYCL_BACKEND_HAS_DPCPP)
    #include "plssvm/backends/autogenerated/DPCPP/detail/constants.hpp"
    #include "plssvm/backends/autogenerated/DPCPP/detail/device_ptr.hpp"
#endif
#if defined(PLSSVM_SYCL_BACKEND_HAS_HIPSYCL)
    #include "plssvm/backends/autogenerated/hipSYCL/detail/constants.hpp"
    #include "plssvm/backends/autogenerated/hipSYCL/detail/device_ptr.hpp"
#endif
#endif

#include "fmt/chrono.h"  // directly print std::chrono literals with fmt
#include "fmt/core.h"    // fmt::print
#include "fmt/os.h"       // fmt::output_file

#include <algorithm>  // std::all_of, std::min, std::max
#include <chrono>     // std::chrono
#include <cmath>      // std::ceil
#include <cstddef>    // std::size_t
#include <vector>     // std::vector

#include <omp.h>

#include <Eigen/Dense>
#include <Eigen/Eigenvalues>
using namespace Eigen;

namespace plssvm::detail {


gpu_csvm::gpu_csvm(const parameter &params) :
    csvm::csvm{ params } {}

auto gpu_csvm::predict(const std::vector<std::vector<real_type>> &points) -> std::vector<real_type> {
    using namespace plssvm::operators;

    PLSSVM_ASSERT(data_ptr_ != nullptr, "No data is provided!");  // exception in constructor
    PLSSVM_ASSERT(!data_ptr_->empty(), "Data set is empty!");     // exception in constructor

    // return empty vector if there are no points to predict
    if (points.empty()) {
        return std::vector<real_type>{};
    }

    // sanity checks
    if (!std::all_of(points.begin(), points.end(), [&](const std::vector<real_type> &point) { return point.size() == points.front().size(); })) {
        throw exception{ "All points in the prediction point vector must have the same number of features!" };
    } else if (points.front().size() != data_ptr_->front().size()) {
        throw exception{ fmt::format("Number of features per data point ({}) must match the number of features per predict point ({})!", data_ptr_->front().size(), points.front().size()) };
    } else if (alpha_ptr_ == nullptr) {
        throw exception{ "No alphas provided for prediction!" };
    }

    PLSSVM_ASSERT(data_ptr_->size() == alpha_ptr_->size(), "Sizes mismatch!: {} != {}", data_ptr_->size(), alpha_ptr_->size());  // exception in constructor

    // check if data already resides on the first device
    if (data_d_[0].empty()) {
        setup_data_on_device();
    }

    auto start_time = std::chrono::steady_clock::now();

    std::vector<real_type> out(points.size());

    if (kernel_ == kernel_type::linear) {
        // use faster methode in case of the linear kernel function
        if (w_.empty()) {
            update_w();
        }
        #pragma omp parallel for
        for (typename std::vector<std::vector<real_type>>::size_type i = 0; i < points.size(); ++i) {
            out[i] = transposed<real_type>{ w_ } * points[i] + bias_;
        }
    } else {
        // create result vector on the device
        device_ptr_type out_d{ points.size() + boundary_size_, devices_[0] };
        out_d.memset(0);

        // transform prediction data
        const std::vector<real_type> transformed_data = csvm::transform_data(points, boundary_size_, points.size());
        device_ptr_type point_d{ points[0].size() * (points.size() + boundary_size_), devices_[0] };
        point_d.memset(0);
        point_d.memcpy_to_device(transformed_data, 0, transformed_data.size());

        // create the weight vector on the device and copy data
        device_ptr_type alpha_d{ num_data_points_ + THREAD_BLOCK_SIZE, devices_[0] };
        alpha_d.memset(0);
        alpha_d.memcpy_to_device(*alpha_ptr_.get(), 0, num_data_points_);

        const detail::execution_range range({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(num_data_points_) / static_cast<real_type>(THREAD_BLOCK_SIZE))),
                                              static_cast<std::size_t>(std::ceil(static_cast<real_type>(points.size()) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, num_data_points_), std::min<std::size_t>(THREAD_BLOCK_SIZE, points.size()) });

        // perform prediction on the first device
        run_predict_kernel(range, out_d, alpha_d, point_d, points.size());

        out_d.memcpy_to_host(out, 0, points.size());

        // add bias_ to all predictions
        out += bias_;
    }

    auto end_time = std::chrono::steady_clock::now();
    if (print_info_) {
        fmt::print("Predicted {} data points in {}.\n", points.size(), std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time));
    }

    return out;
}

void gpu_csvm::setup_data_on_device() {
    // set values of member variables
    dept_ = num_data_points_ - 1;
    // mixed precision PLSSVM with Tensor Cores
    #if defined(MIXED) && defined (TENSOR)
        if(dept_ % BLOCK_SIZE_F == 0){
            boundary_size_ = 0;
        } else{
            boundary_size_ = BLOCK_SIZE_F - (dept_ % BLOCK_SIZE_F);
        }
        if(num_features_ % (8 * devices_.size()) == 0){
            boundary_size_features_tensor_ = 0;
        } else{
            boundary_size_features_tensor_ = (8 * devices_.size()) - (num_features_ % (8 * devices_.size()));
        }
    #endif
    // double precision PLSSVM with Tensor Cores
    #if !defined(MIXED) && defined (TENSOR)
        if(dept_ % BLOCK_SIZE == 0){
            boundary_size_ = 0;
        } else{
            boundary_size_ = BLOCK_SIZE - (dept_ % BLOCK_SIZE);
        }
        if(num_features_ % (8 * devices_.size()) == 0){
            boundary_size_features_tensor_ = 0;
        } else{
            boundary_size_features_tensor_ = (8 * devices_.size()) - (num_features_ % (8 * devices_.size()));
        } 
    #endif
    // no tensor kernel used
    #if !defined (TENSOR)
       boundary_size_ = static_cast<std::size_t>(THREAD_BLOCK_SIZE * INTERNAL_BLOCK_SIZE);
       boundary_size_features_tensor_ = 0; 
    #endif
    num_rows_ = dept_ + boundary_size_;
    num_cols_ = num_features_ + boundary_size_features_tensor_;
    // fmt::print("num_features: {}, num_cols: {}, num_rows: {}, boundary_size_ {} \n", num_features_, num_cols_, num_rows_, boundary_size_);

    // Data Test: // TODO löschen
    /* double sum_test = 0.0;
    for (size_t i = 0; i < dept_ + 1; ++i) {
        for (size_t k = 0; k < num_features_ -1; ++k) {
            sum_test += std::abs((*data_ptr_)[i][k]);
        }
    }
    printf("sum_test %f and %f\n", sum_test, sum_test/(dept_ + 1)/num_features_);
     */

    /* feature_ranges_.reserve(devices_.size() + 1);
    for (typename std::vector<queue_type>::size_type device = 0; device <= devices_.size(); ++device) {
        feature_ranges_.push_back(device * num_rows_ / devices_.size());
    }
    std::vector<double> fullMatrix((dept_ + boundary_size_) * (dept_ + boundary_size_), 0.0);
    for(int i = 0; i < num_rows_; ++i){
        for(int j = 0; j < num_rows_; ++j){
            double sum = 0.0;
            for(int k = 0; k < num_features_; ++k){
                sum += (*data_ptr_)[i][k]*(*data_ptr_)[j][k];
            }
            if (i == j){
                fullMatrix[i * num_rows_ + j] = sum - q[i] - q[j] + QA_cost_ + cost_;
            } else {
                fullMatrix[i * num_rows_ + j] = sum - q[i] - q[j] + QA_cost_;
            }
        }
    }

    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        const std::size_t num_features = feature_ranges_[device + 1] - feature_ranges_[device];

        const detail::execution_range range_r ({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_ + boundary_size_) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_ + boundary_size_) });


        const std::size_t device_data_size = num_features * (dept_ + boundary_size_);
        data_d_[device] = device_ptr_type{ device_data_size, devices_[device] };
        data_d_[device].memcpy_to_device(fullMatrix.data() + feature_ranges_[device] * (dept_ + boundary_size_), 0, device_data_size);

        const detail::execution_range range_full ({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_ * (dept_ + boundary_size_)) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_ + boundary_size_) });

        data_d_f_[device] = device_ptr_type_float{ device_data_size, devices_[device] };
        run_transformation_kernel_df(device, range_full, data_d_f_[0], data_d_[0]);
    }
    */

    feature_ranges_.reserve(devices_.size() + 1);
    for (typename std::vector<queue_type>::size_type device = 0; device <= devices_.size(); ++device) {
        feature_ranges_.push_back(device * num_cols_ / devices_.size());
    }
    // transform 2D to 1D data
    const std::vector<real_type> transformed_data = csvm::transform_data(*data_ptr_, boundary_size_, dept_, boundary_size_features_tensor_);
    // fmt::print("Hi before setup \n");

    // test section:
    // std::vector<float> transformed_data_f_orig(transformed_data.size());
    // std::vector<float> transformed_data_f_gpu(transformed_data.size());

    #if defined(MIXED)
        std::vector<float> transformed_data_f(transformed_data.size());
#pragma omp parallel for    
        for (size_t i = 0; i < transformed_data.size(); ++i) {
            transformed_data_f[i] = static_cast<float>(transformed_data[i]);
        }
    #endif

#pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        const std::size_t num_features = feature_ranges_[device + 1] - feature_ranges_[device];

        // fmt::print("num_features: {}, num_cols: {}, num_rows: {}, boundary_size_ {} \n", num_features, num_cols_, num_rows_, boundary_size_);

        // initialize data_last on device
        data_last_d_[device] = device_ptr_type{ num_features + boundary_size_, devices_[device] };
        data_last_d_[device].memset(0);
        data_last_d_[device].memcpy_to_device(data_ptr_->back().data() + feature_ranges_[device], 0, num_features);

        #if defined(MIXED)
        const detail::execution_range range_r({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(num_features) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },  // num_features + boundary_size_ ?
                                              { std::min<std::size_t>(THREAD_BLOCK_SIZE, num_features) });

        data_last_d_f_[device] = device_ptr_type_float{ num_features + boundary_size_, devices_[device] };
        data_last_d_f_[device].memset(0);
        run_transformation_kernel_df(device, range_r, data_last_d_f_[device], data_last_d_[device]);
        #endif

        // TODO Check here or in Kernels for integer overflow
        const std::size_t device_data_size = num_features * (dept_ + boundary_size_);
        #if !defined(MIXED)
            data_d_[device] = device_ptr_type{ device_data_size, devices_[device] };
            data_d_[device].memcpy_to_device(transformed_data.data() + feature_ranges_[device] * (dept_ + boundary_size_), 0, device_data_size);
        #endif

        #if defined(MIXED)
        data_d_f_[device] = device_ptr_type_float{ device_data_size, devices_[device] };
        data_d_f_[device].memcpy_to_device(transformed_data_f.data() + feature_ranges_[device] * (dept_ + boundary_size_), 0, device_data_size);
        #endif
    }

    // fmt::print("Setup done \n");

    // debug section
    /*
    int counter = 0;
    float value = 0.0;
    float abs;
    data_d_f_[0].memcpy_to_host(transformed_data_f_gpu.data(), 0, transformed_data.size());
    for (size_t i = 0; i < transformed_data.size()/devices_.size(); ++i)
    {
        if(transformed_data_f_gpu[i] != transformed_data_f_orig[i]){
            counter++;
            abs = (transformed_data_f_gpu[i] >transformed_data_f_orig[i]) ? transformed_data_f_gpu[i] - transformed_data_f_orig[i] : transformed_data_f_orig[i] - transformed_data_f_gpu[i];
            value += abs;
        }
    }
    fmt::print("counter: {} error: {} rel_error: {} size: {} \n", counter, value, value/transformed_data.size(), transformed_data.size());
    */
}

auto gpu_csvm::generate_q_f() -> std::vector<float> {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    std::vector<device_ptr_type_float> q_d_f(devices_.size());

    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        q_d_f[device] = device_ptr_type_float{ dept_ + boundary_size_, devices_[device] };
        q_d_f[device].memset(0);

        // feature splitting on multiple devices
        const detail::execution_range range({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_) });

        run_q_kernel_f(device, range, q_d_f[device], feature_ranges_[device + 1] - feature_ranges_[device]);
    }

    std::vector<float> q_f(dept_);
    device_reduction_f(q_d_f, q_f);

    return q_f;
}

auto gpu_csvm::generate_q() -> std::vector<real_type> {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    std::vector<device_ptr_type> q_d(devices_.size());

    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        q_d[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
        q_d[device].memset(0);

        // feature splitting on multiple devices
        const detail::execution_range range({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_) });

        run_q_kernel(device, range, q_d[device], feature_ranges_[device + 1] - feature_ranges_[device]);
    }

    std::vector<real_type> q(dept_);
    device_reduction(q_d, q);
    // fmt::print("data_ptr sizes: {} \n", data_ptr_->size());

    // n-1 Matrix
    /*
    std::vector<double> fullMatrix(dept_ * dept_);
    for(int i = 0; i < dept_; ++i){
        for(int j = 0; j < dept_; ++j){
            double sum = 0.0;
            for(int k = 0; k < num_features_; ++k){
                sum += (*data_ptr_)[i][k]*(*data_ptr_)[j][k];
            }
            if (i == j){
                fullMatrix[i * dept_ + j] = sum - q[i] - q[j] + QA_cost_ + cost_;
            } else {
                fullMatrix[i * dept_ + j] = sum - q[i] - q[j] + QA_cost_;
            }
        }
    }
    size_t m_size = dept_;
    */

   /*
    // n+1 matrix in float
    Eigen::setNbThreads(16);
    size_t m_size = dept_+2;

    
    std::vector<float> fullMatrix_f((dept_+2) * (dept_+2));
    //std::shared_ptr<std::vector<std::vector<float>>> data_ptr_f_(dept_+2, std::vector<float>(dept_+2));
    std::shared_ptr<std::vector<std::vector<float>>> data_ptr_f_ = std::make_shared<std::vector<std::vector<float>>>(dept_+1, std::vector<float>(num_features_));
    fmt::print("Hi \n");
    float gamma_f = -1.0 / num_features_;
    for(size_t i = 0; i < data_ptr_f_->size(); ++i){
        for(size_t j = 0; j < (*data_ptr_f_)[0].size(); ++j){
            (*data_ptr_f_)[i][j] = static_cast<float>((*data_ptr_)[i][j]); 
        }
    }
    for(int i = 0; i < dept_+1; ++i){
        for(int j = 0; j < dept_+1; ++j){
            float sum = 0.0;
            for(int k = 0; k < num_features_; ++k){
                sum += ((*data_ptr_f_)[i][k]-(*data_ptr_f_)[j][k]) * ((*data_ptr_f_)[i][k]-(*data_ptr_f_)[j][k]);
            }
            if (i == j){
                fullMatrix_f[i * (dept_+2) + j] = exp(gamma_f * sum) + cost_f_;
            } else {
                fullMatrix_f[i * (dept_+2) + j] = exp(gamma_f * sum); // exp(gamma_f * sum)
            }
        }
    }
    for(int i = 0; i < dept_+1; ++i){
        fullMatrix_f[dept_+1+i*(dept_+2)] = 1.0;
        fullMatrix_f[(dept_+1)*(dept_+2) + i] = 1.0;
    }
    fullMatrix_f[(dept_+2)*(dept_+2)-1]= 0.0;

    fmt::print("Hi, wrote Matrix_F! Compute now ... \n");
    MatrixXd mat_f(m_size, m_size);
    for(int i = 0; i < m_size; ++i){
        for(int j = 0; j < m_size; ++j){
            mat_f(i, j) = static_cast<double>(fullMatrix_f[i * m_size + j]);
        }
    }
    JacobiSVD<MatrixXd> svd_f(mat_f);
    double cond_f = svd_f.singularValues()(0) / svd_f.singularValues()(svd_f.singularValues().size()-1);

    fmt::print("Condition float is: {}, max_EW: {}; min_EW: {} \n", cond_f, svd_f.singularValues()(0), svd_f.singularValues()(svd_f.singularValues().size()-1));
    fmt::ostream out_f = fmt::output_file("Eigenvalues_float.txt"); 
    for(size_t i = 0; i < svd_f.singularValues().size(); ++i)
    out_f.print("{} ", svd_f.singularValues()(i));
    

    // n+1 matrix 
    double gamma = -1.0 / num_features_;
    std::vector<double> fullMatrix((dept_+2) * (dept_+2));
    for(int i = 0; i < dept_+1; ++i){
        for(int j = 0; j < dept_+1; ++j){
            double sum = 0.0;
            for(int k = 0; k < num_features_; ++k){
                sum += ((*data_ptr_)[i][k] - (*data_ptr_)[j][k]) * ((*data_ptr_)[i][k]-(*data_ptr_)[j][k]);
            }
            if (i == j){
                fullMatrix[i * (dept_+2) + j] = exp(gamma * sum) + cost_;
            } else {
                fullMatrix[i * (dept_+2) + j] = exp(gamma * sum); //
            }
        }
    }
    for(int i = 0; i < dept_+1; ++i){
        fullMatrix[dept_+1+i*(dept_+2)] = 1.0;
        fullMatrix[(dept_+1)*(dept_+2) + i] = 1.0;
    }
    fullMatrix[(dept_+2)*(dept_+2)-1]= 0.0;
    // size_t m_size = dept_+2; 

    fmt::print("Hi, wrote Matrix_D! Compute now ... \n");
    MatrixXd mat(m_size, m_size);
    for(int i = 0; i < m_size; ++i){
        for(int j = 0; j < m_size; ++j){
            mat(i, j) = fullMatrix[i * m_size + j];
        }
    }
    JacobiSVD<MatrixXd> svd(mat);
    double cond = svd.singularValues()(0) / svd.singularValues()(svd.singularValues().size()-1);

    fmt::print("Condition double is: {}, max_EW: {}; min_EW: {} \n", cond, svd.singularValues()(0), svd.singularValues()(svd.singularValues().size()-1));
    fmt::ostream out = fmt::output_file("Eigenvalues_double.txt");
    fmt::ostream out_error = fmt::output_file("Eigenvalues_diff.txt");  
    for(size_t i = 0; i < svd.singularValues().size(); ++i){
        out.print("{} ", svd.singularValues()(i));
        out_error.print("{} ", svd.singularValues()(i) - svd_f.singularValues()(i));
    }

    
    
    double sum_diff = 0.0;
    double sum_matrix = 0.0;
    // double rel_sum_error = 0.0;
    double tmp_f;
    for(int i = 0; i < m_size; ++i){
        for(int j = 0; j < m_size; ++j){
            tmp_f = static_cast<double>(fullMatrix_f[i * m_size + j]);
            sum_diff += (fullMatrix[i * m_size + j] - tmp_f > 0) ? fullMatrix[i * m_size + j] - tmp_f : tmp_f - fullMatrix[i * m_size + j];
            sum_matrix += fullMatrix[i * m_size + j];
        }
    }
    fmt::print("Diff sum is: {} , Matrix sum is: {} , rel_error is: {} \n", sum_diff, sum_matrix, sum_diff/sum_matrix);
    */
    
    return q;
}

auto gpu_csvm::solver_CG(const std::vector<real_type> &b, const std::size_t imax, const real_type eps, const std::vector<real_type> &q) -> std::vector<real_type> {
    using namespace plssvm::operators;

    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    // Checks and co.
    
    // output für Analyse
    // fmt::ostream out = fmt::output_file("Res_Verlauf.txt"); 
    // fmt::ostream out_z = fmt::output_file("Analyse_zahlen.txt"); 
    

    std::vector<real_type> b_new(b);
    real_type b_res = transposed<double>{ b } * b;

    std::vector<real_type> y(dept_, 0.0);
    std::vector<real_type> x(dept_, 1.0/(num_features_ * dept_));
    std::vector<device_ptr_type> x_d(devices_.size());

    std::vector<real_type> r(dept_, 0.0);
    std::vector<device_ptr_type> r_d(devices_.size());

    #if defined(MIXED)
        std::vector<float> x_f(dept_, 1.0/(num_features_ * dept_));
        std::vector<device_ptr_type_float> x_d_f(devices_.size());
        std::vector<device_ptr_type_float> r_d_f(devices_.size());
        // conversions
        const detail::execution_range range_r ({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_ + boundary_size_) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_) });
    #endif

    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        x_d[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
        x_d[device].memset(0);
        x_d[device].memcpy_to_device(x, 0, dept_);

        r_d[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
        r_d[device].memset(0);

        #if defined(MIXED)
            x_d_f[device] = device_ptr_type_float{ dept_ + boundary_size_, devices_[device] };
            x_d_f[device].memset(0);
            run_transformation_kernel_df(device, range_r, x_d_f[device], x_d[device]);

            r_d_f[device] = device_ptr_type_float{ dept_ + boundary_size_, devices_[device] };
            r_d_f[device].memset(0);
        #endif
    }
    r_d[0].memcpy_to_device(b, 0, dept_);
    #if defined(MIXED)
        run_transformation_kernel_df(0, range_r, r_d_f[0], r_d[0]);

        std::vector<device_ptr_type_float> q_d_f(devices_.size());
    #endif

    // debug
    // fmt::print("exec_range: {} {} size: {} \n", static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_) / static_cast<real_type>(THREAD_BLOCK_SIZE))), std::min<std::size_t>(THREAD_BLOCK_SIZE, dept_), dept_);

    std::vector<device_ptr_type> q_d(devices_.size());
    
    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        q_d[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
        q_d[device].memset(0);
        q_d[device].memcpy_to_device(q, 0, dept_);

        #if defined(MIXED)
        q_d_f[device] = device_ptr_type_float{ dept_ + boundary_size_, devices_[device] };
        q_d_f[device].memset(0);
        run_transformation_kernel_df(device, range_r, q_d_f[device], q_d[device]);
        #endif
        
        // r = Ax (r = b - Ax)
        #if defined(MIXED)
            #if defined(TENSOR)
                run_device_kernel_tf(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
            #endif
            #if !defined(TENSOR)
                run_device_kernel_f(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
            #endif
            run_transformation_kernel_fd(device, range_r, r_d[device], r_d_f[device]);
        #endif
        #if !defined(MIXED)
            #if defined(TENSOR)
                run_device_kernel_td(device, q_d[device], r_d[device], x_d[device], -1);
            #endif
            #if !defined(TENSOR)
                run_device_kernel(device, q_d[device], r_d[device], x_d[device], -1);
            #endif
        #endif
    }
    // device_reduction(r_d, r);
    device_reduction(r_d, r);

    // fmt::print("Hello \n");

    // delta = r.T * r
    real_type delta = transposed<double>{ r } * r;
    const real_type delta0 = delta;
    real_type delta_old = delta;
    // zeta for RU
    double zeta = 0.1;

    std::vector<real_type> Ad(dept_);
    #if !defined(MIXED)
        std::vector<device_ptr_type> Ad_d(devices_.size());
    #endif
    #if defined(MIXED)
        std::vector<float> Ad_f(dept_);
        std::vector<device_ptr_type_float> Ad_d_f(devices_.size());
    #endif

    //std::vector<real_type> Ad_test(dept_);
    // std::vector<device_ptr_type> Ad_d_test(devices_.size());

    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        #if !defined(MIXED)
            Ad_d[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
        #endif
        #if defined(MIXED)
            Ad_d_f[device] = device_ptr_type_float{ dept_ + boundary_size_, devices_[device] };
            run_transformation_kernel_df(device, range_r, r_d_f[device], r_d[device]);
        #endif
        // Ad_d_test[device] = device_ptr_type{ dept_ + boundary_size_, devices_[device] };
    }
    // fmt::print("Hi2 \n");
    std::vector<real_type> d(r);

    // Test section:
    /*
    real_type r_orig = transposed<double>{ r } * r;
    run_transformation_kernel_df(0, range_r, r_d_f[0], r_d[0]);
    device_reduction_f(r_d_f, r_f);
    real_type r_float = transposed<float>{ r_f } *r_f;
    run_transformation_kernel_fd(0, range_r, r_d[0], r_d_f[0]);
    device_reduction(r_d, r);
    real_type r_transformed = transposed<double>{ r } *r;
    fmt::print("orig: {}  float: {} transformed: {} \n", r_orig, r_float, r_transformed);
    */
  
    // run_transformation_kernel_df(0, range_r, r_d_f[0], r_d[0]); // TODO: Add for loop for each device
    
    // run_transformation_kernel_df(0, range_r, x_d_f[0], x_d[0]);

    // std::vector<float> d_f(dept_);
    
    #if defined(MIXED)
        for(typename std::vector<queue_type>::size_type cast_i = 0; cast_i < dept_; ++cast_i)
        {
            Ad_f[cast_i] = static_cast<float>(Ad[cast_i]);
        }
    #endif

    // timing for each CG iteration
    std::chrono::microseconds average_iteration_time{};
    std::chrono::steady_clock::time_point iteration_start_time{};
    const auto output_iteration_duration = [&]() {
        auto iteration_end_time = std::chrono::steady_clock::now();
        auto iteration_duration = std::chrono::duration_cast<std::chrono::milliseconds>(iteration_end_time - iteration_start_time);
        // auto iteration_duration = std::chrono::duration_cast<std::chrono::microseconds>(iteration_end_time - iteration_start_time);
        fmt::print("Done in {}.\n", iteration_duration);
        average_iteration_time += iteration_duration;
    };




    std::size_t run = 0;
    for (; run < imax; ++run) { // imax
        if (print_info_) {
            fmt::print("Start Iteration {} (max: {}) with current residuum {} (target: {}). ", run + 1, imax, delta, eps * eps * delta0);
        }
        iteration_start_time = std::chrono::steady_clock::now();

        // Ad = A * r (q = A * d)
        #pragma omp parallel for
        for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
            #if defined(MIXED)
                Ad_d_f[device].memset(0);
                r_d_f[device].memset(0, dept_);
                #if defined(TENSOR)
                    run_device_kernel_tf(device, q_d_f[device], Ad_d_f[device], r_d_f[device], 1);
                #endif
                #if !defined(TENSOR)
                    run_device_kernel_f(device, q_d_f[device], Ad_d_f[device], r_d_f[device], 1);
                #endif
            #endif
            #if !defined(MIXED)
                Ad_d[device].memset(0);
                r_d[device].memset(0, dept_);
                #if defined(TENSOR)
                    run_device_kernel_td(device, q_d[device], Ad_d[device], r_d[device], 1);
                #endif
                #if !defined(TENSOR)
                    run_device_kernel(device, q_d[device], Ad_d[device], r_d[device], 1);
                #endif               
            #endif
        }
        // update Ad (q)
        #if defined(MIXED)
            device_reduction_f(Ad_d_f, Ad_f);
            for(typename std::vector<queue_type>::size_type cast_i = 0; cast_i < dept_; ++cast_i){
                // d[cast_i] = static_cast<double>(d_f[cast_i]); TEST ONLY!
                Ad[cast_i] = static_cast<double>(Ad_f[cast_i]);
                // x[cast_i] = static_cast<double>(x_f[cast_i]); TEST ONLY!
            } 
        #endif  
        #if !defined(MIXED)
            device_reduction(Ad_d, Ad);
        #endif
        // device_reduction(Ad_d_test, Ad_test);

        
        /*
        double min_r = (Ad[0] > 0) ? Ad[0] : -Ad[0];
        double max_r = (Ad[0] > 0) ? Ad[0] : -Ad[0];
        double abs_tmp;
        float min_r_f = (Ad_f[0] > 0) ? Ad_f[0] : -Ad_f[0];
        float max_r_f = (Ad_f[0] > 0) ? Ad_f[0] : -Ad_f[0];
        float abs_tmp_f;
        std::vector<real_type> testvec_d (dept_, 1);
        real_type r_orig = transposed<double>{ Ad } * testvec_d;
        real_type r_reduction = sum(Ad);
        fmt::print("\n r_diff: {} \n", (r_orig - r_reduction));
        real_type r_mm = transposed<double>{ Ad_test } * testvec_d;
        std::vector<float> testvec_f (dept_, 1);
        real_type r_float = transposed<float>{ Ad_f } *testvec_f;
        for(typename std::vector<queue_type>::size_type cast_i = 0; cast_i < dept_; ++cast_i)
        {
            abs_tmp = (Ad[cast_i] >0) ? Ad[cast_i] : -Ad[cast_i];
            min_r = (min_r < abs_tmp) ? min_r : abs_tmp;
            max_r = (max_r > abs_tmp) ? max_r : abs_tmp;
            abs_tmp_f = (Ad_f[cast_i] >0) ? Ad_f[cast_i] : -Ad_f[cast_i];
            min_r_f = (min_r_f < abs_tmp_f) ? min_r_f : abs_tmp_f;
            max_r_f = (max_r_f > abs_tmp_f) ? max_r_f : abs_tmp_f;
            // d[cast_i] = static_cast<double>(d_f[cast_i]);
            // Ad[cast_i] = static_cast<double>(Ad_f[cast_i]);
            // x[cast_i] = static_cast<double>(x_f[cast_i]);
            Ad_test[cast_i] = static_cast<double>(Ad_f[cast_i]);
        }        
        real_type r_m = transposed<double>{ Ad_test } * testvec_d;
        real_type error_sum_f = (r_orig > r_float) ? r_orig-r_float : r_float-r_orig;
        real_type rel_error_f = error_sum_f / r_orig;
        real_type error_sum_m = (r_orig > r_m) ? r_orig-r_m : r_m-r_orig;
        real_type rel_error_m = error_sum_m / r_orig;
        real_type error_sum_mm = (r_orig > r_mm) ? r_orig-r_mm : r_mm-r_orig;
        real_type rel_error_mm = error_sum_mm / r_orig;
        fmt::print("\n max_ri_i: {} min_r_i: {} max_ri_f: {} min_ri_f: {} orig_r_norm: {}  float_r_norm: {} mixed_r_norm: {} mixed_r_mat_norm: {} \n Summenfehler_O_F: {} relativerFehler_O_F: {} Summenfehler_O_M: {} relativerFehler_O_M: {} Summenfehler_O_MM: {} relativerFehler_O_MM: {} \n", 
        max_r, min_r, max_r_f, min_r_f, r_orig, r_float, r_m, r_mm, error_sum_f, rel_error_f, error_sum_m, rel_error_m, error_sum_mm, rel_error_mm);
        out.print("max_ri_d: {} max_ri_f: {} min_ri_d: {} min_ri_f: {} orig_r_norm: {}  float_r_norm: {} mixed_r_norm: {} mixed_r_mat_norm: {} Summenfehler_O_F: {} relativerFehler_O_F: {} Summenfehler_O_M: {} relativerFehler_O_M: {} Summenfehler_O_MM: {} relativerFehler_O_MM: {} \n", 
        max_r, max_r_f, min_r, min_r_f, r_orig, r_float, r_m, r_mm, error_sum_f, rel_error_f, error_sum_m, rel_error_m, error_sum_mm, rel_error_mm);
        out_z.print("{} {} {} {} {} {} {} {} {} {} {} {} {} {}\n", max_r, max_r_f, min_r, min_r_f, r_orig, r_float, r_m, r_mm, error_sum_f, rel_error_f, error_sum_m, rel_error_m, error_sum_mm, rel_error_mm);
        */



        // (alpha = delta_new / (d^T * q))
        const real_type alpha_cd = delta / (transposed<double>{ d } * Ad);

        #if defined(POLAK_RIBIERE)
            std::vector<real_type>r_old(r); // for alternative Polak-Ribiere
        #endif
        
        if constexpr(CORRECTION_SCHEME == correction_scheme::zero){
            // r -= alpha_cd * Ad (r = r - alpha * q)
            r -= alpha_cd * Ad;
            // (x = x + alpha * d)
            x += alpha_cd * d;
        } else if constexpr(CORRECTION_SCHEME == correction_scheme::NewRScheme){
            // (x = x + alpha * d)
            x += alpha_cd * d;
            if (run % 50 == 49) {
                #pragma omp parallel for
                for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
                    // r = b
                    if(device == 0) {
                        r_d[0].memcpy_to_device(b, 0, dept_);
                        #if defined(MIXED)
                            run_transformation_kernel_df(0, range_r, r_d_f[0], r_d[0]);
                        #endif 
                    } else {
                        // set r to 0
                        r_d[device].memset(0);
                        #if defined(MIXED)
                            r_d_f[device].memset(0);
                        #endif
                    }
                    // r -= A * x
                    #if defined(MIXED)
                        #if defined(TENSOR)
                            run_device_kernel_tf(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
                        #endif
                        #if !defined(TENSOR)
                            run_device_kernel_f(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
                        #endif
                        run_transformation_kernel_fd(device, range_r, r_d[device], r_d_f[device]);
                    #endif
                    #if !defined(MIXED)
                        #if defined(TENSOR)
                            run_device_kernel_td(device, q_d[device], r_d[device], x_d[device], -1);
                        #endif
                        #if !defined(TENSOR)
                            run_device_kernel(device, q_d[device], r_d[device], x_d[device], -1);
                        #endif
                    #endif
                }
                device_reduction(r_d, r);
            } else {
                // r -= alpha_cd * Ad (r = r - alpha * q)
                r -= alpha_cd * Ad;
            }
        }else if constexpr(CORRECTION_SCHEME == correction_scheme::ReliableUpdate){
            if (delta < zeta * b_res) {

                // (x = x + alpha * d)
                x += alpha_cd * d;
                y+= x;

                #if defined(MIXED)
                    for(typename std::vector<queue_type>::size_type cast_i = 0; cast_i < dept_; ++cast_i) {
                        x_f[cast_i] = static_cast<float>(x[cast_i]);
                    }
                    #pragma omp parallel for
                    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
                        x_d_f[device].memcpy_to_device(x_f, 0, dept_);
                    }
                #endif
                #if !defined(MIXED)
                    #pragma omp parallel for
                    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
                        // x_d_f[device].memcpy_to_device(x_f, 0, dept_);
                        x_d[device].memcpy_to_device(x, 0, dept_);
                    }
                #endif

                fmt::print("correction in iteration {} \n ", run);

                #pragma omp parallel for
                for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
                    if(device == 0) {
                        // r = b
                        r_d[0].memcpy_to_device(b_new, 0, dept_);
                        #if defined(MIXED)
                            run_transformation_kernel_df(0, range_r, r_d_f[0], r_d[0]);
                        #endif
                    } else {
                        // set r to 0
                        r_d[device].memset(0);
                        #if defined(MIXED)
                            r_d_f[device].memset(0);
                        #endif
                    }
                    // r -= A * x
                    #if defined(MIXED)
                        #if defined(TENSOR)
                            run_device_kernel_tf(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
                        #endif
                        #if !defined(TENSOR)
                            run_device_kernel_f(device, q_d_f[device], r_d_f[device], x_d_f[device], -1);
                        #endif
                        run_transformation_kernel_fd(device, range_r, r_d[device], r_d_f[device]);
                    #endif
                    #if !defined(MIXED)
                        #if defined(TENSOR)
                            run_device_kernel_td(device, q_d[device], r_d[device], x_d[device], -1);
                        #endif
                        #if !defined(TENSOR)
                            run_device_kernel(device, q_d[device], r_d[device], x_d[device], -1);
                        #endif
                    #endif
                }
                device_reduction(r_d, r);
                b_new = r;
                b_res = transposed<double>{ r } * r;
                
                std::fill(x.begin(), x.end(), 0.0);
                // zeta = 0.05;
            } else {
                // r -= alpha_cd * Ad (r = r - alpha * q)
                r -= alpha_cd * Ad;
                // (x = x + alpha * d)
                x += alpha_cd * d;
                // zeta+= 0.01;
            }
        }

        // (delta = r^T * r)
        delta_old = delta;
        delta = transposed<double>{ r } * r;
        // if we are exact enough stop CG iterations
        if (delta <= eps * eps * delta0) {
        // if (delta <= 0.1) {
            if (print_info_) {
                output_iteration_duration();
                // out.print("{}", delta);
            }
            break;
        }

        // (beta = delta_new / delta_old) (Fletcher Reeves)  alt. beta = r^T*(r - r_old)/ delta_old (Polak-Ribiere)
        #if defined(POLAK_RIBIERE)
            const real_type beta = transposed<double>{ r} * (r-r_old) / delta_old;
        #endif
        #if !defined(POLAK_RIBIERE)
            const real_type beta = delta / delta_old;
        #endif
        
        
        // d = beta * d + r
        d = r + beta * d;

        // r_d = d
        #pragma omp parallel for
        for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
            r_d[device].memcpy_to_device(d, 0, dept_);
            #if defined(MIXED)
                run_transformation_kernel_df(device, range_r, r_d_f[device], r_d[device]); // comment out for non
            #endif
        }

        if (print_info_) {
            output_iteration_duration();
            // out.print("{} ", delta);
        }
    }
    if (print_info_) {
        fmt::print("Finished after {} iterations with a residuum of {} (target: {}) and an average iteration time of {}.\n",
                   std::min(run + 1, imax),
                   delta,
                   eps * eps * delta0,
                   average_iteration_time / std::min(run + 1, imax));
                   #if defined(RUNTIME_TEST)
                   time_counter = average_iteration_time / (run);
                   #endif
    }
    y+= x;

    #if defined(RUNTIME_TEST)
    konvergenz_counter = run + 1;
    #endif

    return std::vector<real_type>(y.begin(), y.begin() + dept_);
}

void gpu_csvm::update_w() {
    w_.resize(num_features_);
    #pragma omp parallel for
    for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
        // feature splitting on multiple devices
        const std::size_t num_features = feature_ranges_[device + 1] - feature_ranges_[device];

        // create the w vector on the device
        device_ptr_type w_d = device_ptr_type{ num_features, devices_[device] };
        // create the weight vector on the device and copy data
        device_ptr_type alpha_d{ num_data_points_ + THREAD_BLOCK_SIZE, devices_[device] };
        alpha_d.memcpy_to_device(*alpha_ptr_.get(), 0, num_data_points_);

        const detail::execution_range range({ static_cast<std::size_t>(std::ceil(static_cast<real_type>(num_features) / static_cast<real_type>(THREAD_BLOCK_SIZE))) },
                                            { std::min<std::size_t>(THREAD_BLOCK_SIZE, num_features) });

        // calculate the w vector on the device
        run_w_kernel(device, range, w_d, alpha_d, num_features);
        device_synchronize(devices_[device]);

        // copy back to host memory
        w_d.memcpy_to_host(w_.data() + feature_ranges_[device], 0, num_features);
    }
}

void gpu_csvm::run_device_kernel(const std::size_t device, const device_ptr_type &q_d, device_ptr_type &r_d, const device_ptr_type &x_d, const real_type add) {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    const auto grid = static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_) / static_cast<real_type>(boundary_size_)));
    const detail::execution_range range({ grid, grid }, { THREAD_BLOCK_SIZE, THREAD_BLOCK_SIZE });

    run_svm_kernel(device, range, q_d, r_d, x_d, add, feature_ranges_[device + 1] - feature_ranges_[device]);
}

void gpu_csvm::run_device_kernel_td(const std::size_t device, const device_ptr_type &q_d, device_ptr_type &r_d, const device_ptr_type &x_d, const real_type add) {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    const auto grid = static_cast<std::size_t>(num_rows_ / BLOCK_SIZE);
    const detail::execution_range range({ grid, grid }, { WARP_SIZE, WARPS_PER_BLOCK });

    run_svm_kernel_td(device, range, q_d, r_d, x_d, add, feature_ranges_[device + 1] - feature_ranges_[device]);
}

void gpu_csvm::run_device_kernel_f(const std::size_t device, const device_ptr_type_float &q_d, device_ptr_type_float &r_d, const device_ptr_type_float &x_d, const float add) {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    const auto grid = static_cast<std::size_t>(std::ceil(static_cast<real_type>(dept_) / static_cast<real_type>(boundary_size_)));
    const detail::execution_range range({ grid, grid }, { THREAD_BLOCK_SIZE, THREAD_BLOCK_SIZE });

    run_svm_kernel_f(device, range, q_d, r_d, x_d, add, feature_ranges_[device + 1] - feature_ranges_[device]);
}

void gpu_csvm::run_device_kernel_tf(const std::size_t device, const device_ptr_type_float &q_d, device_ptr_type_float &r_d, const device_ptr_type_float &x_d, const float add) {
    PLSSVM_ASSERT(dept_ != 0, "dept_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(boundary_size_ != 0, "boundary_size_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_rows_ != 0, "num_rows_ not initialized! Maybe a call to setup_data_on_device() is missing?");
    PLSSVM_ASSERT(num_cols_ != 0, "num_cols_ not initialized! Maybe a call to setup_data_on_device() is missing?");

    const auto grid = static_cast<std::size_t>(num_rows_ / BLOCK_SIZE_F);
    const detail::execution_range range({ grid, grid }, { WARP_SIZE, WARPS_PER_BLOCK_F });

    run_svm_kernel_tf(device, range, q_d, r_d, x_d, add, feature_ranges_[device + 1] - feature_ranges_[device]);
}

void gpu_csvm::device_reduction(std::vector<device_ptr_type> &buffer_d, std::vector<real_type> &buffer) {
    using namespace plssvm::operators;

    device_synchronize(devices_[0]);
    buffer_d[0].memcpy_to_host(buffer, 0, buffer.size());

    if (devices_.size() > 1) {
        std::vector<real_type> ret(buffer.size());
        for (typename std::vector<queue_type>::size_type device = 1; device < devices_.size(); ++device) {
            device_synchronize(devices_[device]);
            buffer_d[device].memcpy_to_host(ret, 0, ret.size());

            buffer += ret;
        }

        #pragma omp parallel for
        for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
            buffer_d[device].memcpy_to_device(buffer, 0, buffer.size());
        }
    }
}

void gpu_csvm::device_reduction_f(std::vector<device_ptr_type_float> &buffer_d, std::vector<float> &buffer) {
    using namespace plssvm::operators;

    device_synchronize(devices_[0]);
    buffer_d[0].memcpy_to_host(buffer, 0, buffer.size());

    if (devices_.size() > 1) {
        std::vector<float> ret(buffer.size());
        for (typename std::vector<queue_type>::size_type device = 1; device < devices_.size(); ++device) {
            device_synchronize(devices_[device]);
            buffer_d[device].memcpy_to_host(ret, 0, ret.size());

            buffer += ret;
        }

        #pragma omp parallel for
        for (typename std::vector<queue_type>::size_type device = 0; device < devices_.size(); ++device) {
            buffer_d[device].memcpy_to_device(buffer, 0, buffer.size());
        }
    }
}

/*
// explicitly instantiate template class depending on available backends
#if defined(PLSSVM_HAS_CUDA_BACKEND)
// template class gpu_csvm<float, ::plssvm::cuda::detail::device_ptr<float>, int>;
template class gpu_csvm<double, ::plssvm::cuda::detail::device_ptr<double>, int>;
#endif
#if defined(PLSSVM_HAS_HIP_BACKEND)
// template class gpu_csvm<float, ::plssvm::hip::detail::device_ptr<float>, int>;
template class gpu_csvm<double, ::plssvm::hip::detail::device_ptr<double>, int>;
#endif
#if defined(PLSSVM_HAS_OPENCL_BACKEND)
// template class gpu_csvm<float, ::plssvm::opencl::detail::device_ptr<float>, ::plssvm::opencl::detail::command_queue>;
template class gpu_csvm<double, ::plssvm::opencl::detail::device_ptr<double>, ::plssvm::opencl::detail::command_queue>;
#endif
#if defined(PLSSVM_HAS_SYCL_BACKEND)
#if defined(PLSSVM_SYCL_BACKEND_HAS_DPCPP)
// template class gpu_csvm<float, ::plssvm::dpcpp::detail::device_ptr<float>, std::unique_ptr<::plssvm::dpcpp::detail::sycl::queue>>;
template class gpu_csvm<double, ::plssvm::dpcpp::detail::device_ptr<double>, std::unique_ptr<::plssvm::dpcpp::detail::sycl::queue>>;
#endif
#if defined(PLSSVM_SYCL_BACKEND_HAS_HIPSYCL)
// template class gpu_csvm<float, ::plssvm::hipsycl::detail::device_ptr<float>, std::unique_ptr<::plssvm::hipsycl::detail::sycl::queue>>;
template class gpu_csvm<double, ::plssvm::hipsycl::detail::device_ptr<double>, std::unique_ptr<::plssvm::hipsycl::detail::sycl::queue>>;
#endif
#endif */

}  // namespace plssvm::detail
